{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db7e9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "\n",
    "# =========================\n",
    "# PARAMETERS\n",
    "# =========================\n",
    "MODEL_PATH = \"asl_final_model.h5\"\n",
    "IMG_SIZE = 128  # Must match training\n",
    "CLASSES = ['A','B','C','D','E','F','G','H','I','J','K',\n",
    "           'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "           'W','X','Y','Z','del','nothing','space']\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# =========================\n",
    "# MEDIAPIPE HANDS SETUP\n",
    "# =========================\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# WEBCAM & SENTENCE BUILDER\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(0)\n",
    "sentence = \"\"\n",
    "letter_queue = deque(maxlen=15)  # Smooth prediction over last 15 frames\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            h, w, c = frame.shape\n",
    "            x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "            y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "            x_min, x_max = int(min(x_coords)*w), int(max(x_coords)*w)\n",
    "            y_min, y_max = int(min(y_coords)*h), int(max(y_coords)*h)\n",
    "\n",
    "            margin = 20\n",
    "            x_min, y_min = max(x_min-margin,0), max(y_min-margin,0)\n",
    "            x_max, y_max = min(x_max+margin,w), min(y_max+margin,h)\n",
    "\n",
    "            hand_img = frame[y_min:y_max, x_min:x_max]\n",
    "            if hand_img.size > 0:\n",
    "                hand_img = cv2.resize(hand_img, (IMG_SIZE, IMG_SIZE))\n",
    "                hand_img = hand_img.astype(\"float32\") / 255.0\n",
    "                hand_img = np.expand_dims(hand_img, axis=0)\n",
    "\n",
    "                pred = model.predict(hand_img, verbose=0)\n",
    "                label = CLASSES[np.argmax(pred)]\n",
    "                conf = np.max(pred)\n",
    "\n",
    "                letter_queue.append(label)\n",
    "\n",
    "                # Take the most frequent letter in queue\n",
    "                if len(letter_queue) == letter_queue.maxlen:\n",
    "                    final_letter = max(set(letter_queue), key=letter_queue.count)\n",
    "                    if final_letter == \"space\":\n",
    "                        sentence += \" \"\n",
    "                    elif final_letter == \"del\":\n",
    "                        sentence = sentence[:-1]\n",
    "                    elif final_letter != \"nothing\":\n",
    "                        sentence += final_letter\n",
    "                    letter_queue.clear()  # Reset queue after confirming letter\n",
    "\n",
    "                cv2.putText(frame, f\"Letter: {label} ({conf:.2f})\", (x_min, y_min-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    # Display sentence\n",
    "    cv2.putText(frame, f\"Sentence: {sentence}\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"ASL Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605a2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# 1. LOAD MODEL\n",
    "# ======================\n",
    "MODEL_PATH = \"asl_final_model.h5\"\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Load class labels dynamically (same as training)\n",
    "dataset_path = \"asl_alphabet_train\"  # your dataset folder\n",
    "class_labels = sorted(os.listdir(dataset_path))  # all subfolders\n",
    "print(\"✅ Classes:\", class_labels)\n",
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ======================\n",
    "# 2. MEDIAPIPE SETUP\n",
    "# ======================\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 3. SETTINGS\n",
    "# ======================\n",
    "frame_buffer = deque(maxlen=15)   # smooth prediction over last 20 frames\n",
    "sentence = \"\"\n",
    "CONF_THRESHOLD = 0.7\n",
    "\n",
    "# ======================\n",
    "# 4. PREDICTION FUNCTION\n",
    "# ======================\n",
    "def predict_hand(frame, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    if w <= 0 or h <= 0:\n",
    "        return None, None\n",
    "\n",
    "    hand_img = frame[y:y+h, x:x+w]\n",
    "    if hand_img.size == 0:\n",
    "        return None, None\n",
    "\n",
    "    hand_img = cv2.resize(hand_img, (IMG_SIZE, IMG_SIZE))\n",
    "    hand_img = hand_img.astype(\"float32\") / 255.0\n",
    "    hand_img = np.expand_dims(hand_img, axis=0)\n",
    "\n",
    "    preds = model.predict(hand_img, verbose=0)\n",
    "    class_id = int(np.argmax(preds))\n",
    "    confidence = float(preds[0][class_id])\n",
    "    return class_labels[class_id], confidence\n",
    "\n",
    "# ======================\n",
    "# 5. WEBCAM LOOP\n",
    "# ======================\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev_time = time.time()\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = int(min([lm.x for lm in hand_landmarks.landmark]) * w) - 20\n",
    "            y_min = int(min([lm.y for lm in hand_landmarks.landmark]) * h) - 20\n",
    "            x_max = int(max([lm.x for lm in hand_landmarks.landmark]) * w) + 20\n",
    "            y_max = int(max([lm.y for lm in hand_landmarks.landmark]) * h) + 20\n",
    "\n",
    "            x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "            x_max, y_max = min(w, x_max), min(h, y_max)\n",
    "\n",
    "            bbox = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "\n",
    "            label, conf = predict_hand(frame, bbox)\n",
    "\n",
    "            if label is not None and conf >= CONF_THRESHOLD:\n",
    "                frame_buffer.append(label)\n",
    "\n",
    "                # Take majority vote when buffer is full\n",
    "                if len(frame_buffer) == frame_buffer.maxlen:\n",
    "                    final_label = max(set(frame_buffer), key=frame_buffer.count)\n",
    "\n",
    "                    if final_label == \"space\":\n",
    "                        sentence += \" \"\n",
    "                    elif final_label == \"del\":\n",
    "                        sentence = sentence[:-1]\n",
    "                    elif final_label != \"nothing\":\n",
    "                        sentence += final_label\n",
    "\n",
    "                    frame_buffer.clear()\n",
    "\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{label} ({conf:.2f})\", (x_min, y_min - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show sentence\n",
    "    cv2.putText(frame, f\"Sentence: {sentence}\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 0), 2)\n",
    "\n",
    "    # FPS counter\n",
    "    curr_time = time.time()\n",
    "    fps = 1 / (curr_time - prev_time)\n",
    "    prev_time = curr_time\n",
    "    cv2.putText(frame, f\"FPS: {int(fps)}\", (10, 90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"ASL Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
